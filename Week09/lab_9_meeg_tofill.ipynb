{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69f0807",
   "metadata": {},
   "source": [
    "<img src=\"https://www.epfl.ch/about/overview/wp-content/uploads/2020/07/logo-epfl-1024x576.png\" style=\"padding-right:10px;width:140px;float:left\"></td>\n",
    "<h2 style=\"white-space: nowrap\">Neural Signal and Signal Processing (NX-421)</h2>\n",
    "<hr style=\"clear:both\"></hr>\n",
    "<h1><font color='black'>M/EEG: Introduction and basic analysis</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1b330",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 0px;'>\n",
    "<font size = \"3\">In this lab we will introduce you to the basics of working with electroencephalography (EEG) and magnetoencephalography (MEG) data. We will cover some introductory visualizations and basic analysis techniques like Event related Potentials (ERP) and Power Spectral Density (PSD). </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbbe15",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> M/EEG </b></p>\n",
    "<p style='text-indent: 0px;'><font size = \"3\">Electrophysiological neuroimaging explores the relationship between neural activity in the brain and electromagnetic field signals from the brain. This phenomenon is studied using two popular neuroimaging methods based on electromagnetic fields known as electroencephalography (EEG) and magnetoencephalography (MEG) \"M/EEG\".</font>\n",
    "<br>\n",
    "</p></span>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> MNE </b></p>\n",
    "<p style='text-indent: 0px;'>\n",
    "<font size = \"3\">In this lab tutorial we will use MNE-Python, an open-source Python package for exploring, visualizing, and analyzing human neurophysiological data: MEG, EEG,and more. Checkout this <a href=\"https://www.frontiersin.org/articles/10.3389/fnins.2013.00267/full\" target=\"_blank\">paper</a>, to know more what MNE-Python can do!</p></font>\n",
    "\n",
    "<font size = \"3\">You can also access the MNE documentation <a href=\"https://mne.tools/stable/generated/mne.io.Raw.html\" target=\"_blank\">here</a></p></font>\n",
    "    \n",
    "<font size = \"2\">Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, and Matti S. Hämäläinen. MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience, 7(267):1–13, 2013.</font>\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing requirements \n",
    "import numpy as np\n",
    "import mne\n",
    "import os.path as op\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "# Importing sample dataset from Mmne_dataNE\n",
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = (sample_data_folder / 'MEG' / 'sample' /\n",
    "                        'sample_audvis_filt-0-40_raw.fif')\n",
    "raw = mne.io.read_raw_fif(sample_data_raw_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5ff49",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-center: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 2px;'> <font size = \"3\">By default we already have some basic information about the dataset we are loading. Here, it gives us the information about projections items in the file along with recorded dataset. We will understand them in more detail later. Let's first try to know more simpler details about the loaded dataset.</font>\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting information\n",
    "info = mne.io.read_info(sample_data_raw_file)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f80a11",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-center: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 0px;'><font size = \"3\">As you can see, the info object keeps track of a lot of information about the data, recording system and the experiment design. Let's have a look at some the necessary information.\n",
    "<br> </font>\n",
    "</p></span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the sampling rate\n",
    "print('Sampling rate:', raw.info['sfreq'], 'Hz')\n",
    "\n",
    "# Check the size of the data matrix\n",
    "print('%s Channels ' % raw.info['nchan'], '%s Samples' % len(raw.times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8e3d3",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-center: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 2px;'> <font size = \"3\">We now know some basic information about the dataset like sampling rate and total number of channels. Let us now extract the MEG and EEG data from the dataset and create separate data structures for both.</font>\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb52a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the MEG and EEG data from the raw data object\n",
    "meg_only = raw.copy().pick_types(meg=True)\n",
    "eeg_only = raw.copy().pick_types(meg=False, eeg=True)\n",
    "\n",
    "# Display the data information (channels x samples)\n",
    "print('%s MEG channels ' % meg_only.info['nchan'], '%s samples' % len(meg_only.times))\n",
    "print('%s EEG channels ' % eeg_only.info['nchan'], '%s samples' % len(eeg_only.times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ae3c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, let's visualize a single channel data\n",
    "# Specify details\n",
    "sfreq = raw.info['sfreq']\n",
    "chan_num = 10\n",
    "start_pt = 1\n",
    "end_pt = 5\n",
    "\n",
    "# For MEG data \n",
    "data_meg, times_meg = meg_only[chan_num, int(sfreq * start_pt):int(sfreq * end_pt)]\n",
    "\n",
    "# For EEG data \n",
    "data_eeg, times_eeg = eeg_only[chan_num, int(sfreq * start_pt):int(sfreq * end_pt)]\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(311)\n",
    "ax.set_title(\"Single channel MEG data\")\n",
    "ax.plot(times_meg, data_meg.T, 'b')\n",
    "\n",
    "ax = plt.subplot(313)\n",
    "ax.set_title(\"Single channel EEG data\")\n",
    "ax.plot(times_eeg, data_eeg.T, 'g')\n",
    "plt.show()\n",
    "\n",
    "# Notice the change in the scalings factors for EEG and MEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c6e5b",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-center: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 0px;'> <font size = \"3\">Now that we have some idea what M/EEG looks like at channel level, Let us now have a look at more channels. For this we will use the plotting from MNE library. \n",
    "    <br>\n",
    "    <br>\n",
    "    Note-You can change the duration and n_channels arguments to play around with the dataset. Also, for more details visit the MNE-Python webpage to learn more about different plotting arguments.\n",
    "<br> </font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEG\n",
    "plt.ion() \n",
    "meg_only.plot(duration=5, n_channels=10)\n",
    "\n",
    "#EEG\n",
    "eeg_only.plot(duration=5, n_channels=10)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911df304",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> Montages </b></p>\n",
    "<p style='text-indent: 0px;'><font size = \"3\">In the context of electrophysiological data, montages refer to the sensor positions in 3D (x, y, z in meters), which can be assigned to existing M/EEG data. By specifying the locations of sensors relative to the brain, Montages play an important role in computing the forward solution and inverse estimates of the neural activity.</font>\n",
    "    \n",
    "<br>\n",
    "\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Let's have a look at the montage used for the EEG data in the loaded sample dataset and how it is represented in the 2D and 3D space. </font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the montage\n",
    "easycap_montage = mne.channels.make_standard_montage('easycap-M10')\n",
    "\n",
    "# 2D visualization \n",
    "fig1 = easycap_montage.plot()  # 2D\n",
    "\n",
    "# 3D visualization \n",
    "fig2 = easycap_montage.plot(kind='3d', show=False)  # 3D\n",
    "fig2 = fig2.gca().view_init(azim=70, elev=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414814",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-center: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 2px;'> <font size = \"3\"> We now know some qualitative and quantitative information about the sample dataset. Before we go into the next part, let's have a quick understanding of the task and experiment design of this dataset. \n",
    "    <br> \n",
    "    <br>\n",
    "    In this experiment, checkerboard patterns were presented to the subject into the left and right visual field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face. Six types of stimulus were recorded in the event information for response to visual stimulus (left and right), auditory stimulus (left and right), face stimulus, and button press.</font>\n",
    "<br>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d47a0f",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> Epochs  </b></p>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Epochs are a data structure for representing and analyzing equal-duration chunks of the EEG/MEG signal. Epochs are most often used to represent data that is time-locked to repeated experimental events (such as stimulus onsets or subject button presses), but can also be used for storing sequential or overlapping frames of a continuous signal e.g., for analysis of resting-state activity. </font>\n",
    "<br>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Let's use the in-built MNE functions for extracting events and corresponding epochs from the dataset and visualize them.</font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3611683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the raw data again, but cropped for first 120 seconds. \n",
    "raw_cropped = mne.io.read_raw_fif(sample_data_raw_file, verbose=False).crop(tmax=120)\n",
    "\n",
    "# We will only focus on EEG data present in the dataset, so extracting the same in raw_cropped_eeg\n",
    "raw_cropped_eeg = raw.pick(['eeg', 'eog']).load_data()\n",
    "#raw_cropped_eeg.info\n",
    "\n",
    "# Extract the events tagged by the stimulus channel (STI 014) already present in the data\n",
    "events = mne.find_events(raw_cropped, stim_channel='STI 014')\n",
    "\n",
    "# Creating an event dictionary. Contains the type of stimulus for each event tagged\n",
    "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,'visual/right': 4, 'face': 5, 'button': 32}\n",
    "epochs = mne.Epochs(raw_cropped_eeg, events, tmin=-0.3, tmax=0.7, event_id=event_dict, preload=True)\n",
    "print(epochs.event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the first 5 epochs and with their event id\n",
    "plt.ion() \n",
    "fig_allepochs = epochs.plot(n_epochs=5, events=events)\n",
    "plt.ioff() \n",
    "\n",
    "# Note the duration of each epoch is specified when we extract the epochs as \"tmin\" and \"tmax\" arguments. Event\n",
    "# names (and ids) are specified under \"event_id\" argument. In this figure, for the first 5 epochs, different events\n",
    "# are marked with different color with their id indicated on top of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d049e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now count the number of epochs present in each event category in the loaded dataset \n",
    "# (Note- in the \"raw_cropped\" variable, we only extracted the first 120 seconds of data (tmax = 120). You can modify \n",
    "# this and decrease/increase the time duration to plot)\n",
    "\n",
    "### YOUR CODE\n",
    "\n",
    "# auditory_left_epochs = \n",
    "# auditory_right_epochs = \n",
    "# visual_left_epochs = \n",
    "# visual_right_epochs = \n",
    "# faces_epochs = \n",
    "# button_press =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c08601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know the events present in the data set and their ids. Let's now try to plot th epochs of only one \n",
    "# type of event i.e., face. \n",
    "\n",
    "# Make sure to already compute the faces_epochs in the previous cell\n",
    "\n",
    "plt.ion() \n",
    "faces_count = len(faces_epochs) \n",
    "catch_faces = mne.pick_events(events, include=[5])\n",
    "epochs['face'].plot(n_epochs=faces_count, events=catch_faces, event_id=event_dict, event_color=dict(face='blue'))\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732f02f8",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> Event Related Potential (ERP)  </b></p>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Event Related Potentials (ERPs) are very small voltages generated in the brain structures in response to specific events or stimuli. They are changes in the M/EEG signal that are time locked to sensory, motor or cognitive events that provide safe and non-invasive approach to study psychophysiological correlates of mental processes. It might also be useful to have quick recap of ERPs from your lecture slides and additionally to know more about what are different types of ERP waveforms from this research article: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3016705/\" target=\"_blank\">Event-related potential: An overview</a></p></font>\n",
    "\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Let's extract and visualize the ERPs for different events from the EEG data in use for this part.</font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and plotting the epochs for auditory(left), visual(left), and faces conditions\n",
    "#\n",
    "# Note that for the plot below:\n",
    "# Time duration = -0.3 to 0.7 (we decided this while extracting the epochs)\n",
    "# Channels: 59 EEG channels (we are only using the EEG data here, i.e. raw_cropped_eeg )\n",
    "# N_average = # of extracted epochs for each event type\n",
    "# Scalp maps (top left) : topography of scalp potentials, color-coded to different channels\n",
    "\n",
    "l_aud = epochs['auditory/left'].average(); fig_aud = l_aud.plot(spatial_colors=True)\n",
    "l_vis = epochs['visual/left'].average(); fig_vis = l_vis.plot(spatial_colors=True)\n",
    "\n",
    "# Plot the ERPs for 'auditory/right', 'visual/right', and 'faces' event\n",
    "\n",
    "### YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ERP analysis we can also visualize the scalp topographies as they vary with time, specifically for baseline, \n",
    "# stimulus onset, and post-stimulus period. Here we will use the MNE functionality to visualize the joint ERP and \n",
    "# time-specific scalp topographies. \n",
    "\n",
    "fig_l_aud_joint = l_aud.plot_joint(times=[-0.15, 0, 0.18, 0.3, 0.5])\n",
    "fig_l_vis_joint = l_vis.plot_joint(times=[-0.15, 0, 0.18, 0.3, 0.5])\n",
    "\n",
    "# Plot the joint ERPs-scalp maps for 'auditory/right', 'visual/right', and 'faces' events.\n",
    "### YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dcfbb",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> In ERP analysis, comparing different conditions is a popular approach among researchers. As ERPs capture the neural activity related to both sensory and cognitive processes, they can be contrasted against different conditions to evaluate how the response of the brain is condition dependent. </font>\n",
    "<br>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Let's use MNE's ERP comparing functionality to compare the ERPs for different events from the EEG data in use for this part.</font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac15134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a dictionary of the conditions we want to study.\n",
    "# Here for example auditory (left) and visual(left). We also specify number of EEG channels \n",
    "# we want to combine by averaging. \n",
    "\n",
    "events_to_compare = dict(auditory_left=l_aud, visual_left=l_vis)\n",
    "picks = [f'EEG 0{n}' for n in range(10, 40)]\n",
    "mne.viz.plot_compare_evokeds(events_to_compare, picks=picks, combine='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise-1\n",
    "\n",
    "# Following the same comparison as above, plot and compare the following conditions\n",
    "# (1) left visual vs. right visual \n",
    "# (2) left auditory vs. right auditory\n",
    "# (3) left auditory, left visual, and faces\n",
    "# (4) right auditory, right visual, and faces\n",
    "#\n",
    "# Note- Use 30 EEG channels and use the combining metric as mean. Also, just for your understanding, \n",
    "# attempt to identify the ERP waveforms associated to each of the comparisons. Good time to revisit the \n",
    "# research article mentioned above on ERP overview.\n",
    "\n",
    "### YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c02bb",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'>\n",
    "<b> Power Spectral Density (PSD)  </b></p>\n",
    "<p style='text-indent: 0px;'><font size = \"3\">As you know from your lecture notes, Power Spectral Density (PSD) is the measure of signal's power content versus frequency. In M/EEG data, PSD is typically used to characterize the spectral content (i.e. frequency-related information) present in the data. It is a useful measure to identify frequencies of interest, noise, and artifacts present in our data </font>\n",
    "<br>\n",
    "<p style='text-indent: 0px;'><font size = \"3\"> Let's use the in-built MNE functions for computing and visualizing the PSD plots of the MEG and EEG data.</font>\n",
    "\n",
    "<font size = \"3\">Have a look at the plot_psd() documentation <a href=\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.plot_psd\" target=\"_blank\">here</a></p></font>\n",
    "</p></span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42385ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD plot of MEG data (Gradiometer and Magnetometer, all channels, default parameters)\n",
    "meg_only.plot_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD plot of EEG data (all channels, default parameters)\n",
    "eeg_only.plot_psd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot the average PSD across all the channels for the first 200 seconds of data and \n",
    "# in the frequency range of 0 to 30 hz. \n",
    "\n",
    "# Check the documentation of MNE's plot_psd() function and utilize the correct arguments to visulize your results\n",
    "\n",
    "### YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f12265",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid #darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='text-indent: 0px;'><font size = \"3\">With the help of PSD plots, we know the frequency domain representation of the sample M/EEG dataset. Let us know visualize the same for our epoched dataset (only EEG data).  </font>\n",
    "<br>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the already created epochs object from previous part with the compute_psd() function with frequency \n",
    "# range of 0-60 Hz.\n",
    "epo_spectrum = epochs.compute_psd(fmin=0, fmax=60)\n",
    "\n",
    "# Print the epoch information (# epochs x  # channels x # frequencies)\n",
    "print(epo_spectrum)\n",
    "\n",
    "# Plot the power spectra plot of the epoch object\n",
    "epo_spectrum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a43ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for a specific stimuli (for example auditory(left))\n",
    "\n",
    "# Extract auditory (left) epochs from the already computed epochs data object.\n",
    "evoked_auditory = epochs['auditory/left'].average()\n",
    "\n",
    "# Print the information of the extracted epochs for auditory (left) stimulus \n",
    "print(evoked_auditory)\n",
    "\n",
    "# Compute the PSD for the extracted epochs \n",
    "evoked_auditory_spectrum = evoked_auditory.compute_psd()\n",
    "\n",
    "# Plot the power spectra plot\n",
    "evoked_auditory_spectrum.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c754a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualize the interpolated scalp topography of power specific to a stimuli, in all frequency bands.\n",
    "# Specifying frequency bands\n",
    "\n",
    "bands = {'Delta (0-4 Hz)': (0, 4), 'Theta (4-8 Hz)': (4, 8), 'Alpha (8-12 Hz)': (8, 12), 'Beta (12-30 Hz)': (12, 30),\n",
    "         'Gamma (30-45 Hz)': (30, 45)};\n",
    "plt.ion()\n",
    "evoked_auditory_spectrum.plot_topomap(bands = bands, ch_type='eeg', normalize=True, agg_fun=np.median)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise-2\n",
    "\n",
    "# Following the above, compute the evoked spectrum and plot the scalp topography of all the stimulus conditions i.e. \n",
    "# (1) visual stimulus (left and right), \n",
    "# (2) auditory stimulus (left and right), and \n",
    "# (3) face stimulus\n",
    "# \n",
    "# Follow the above example for auditory (left) \n",
    "\n",
    "### YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
